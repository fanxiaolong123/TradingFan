#!/usr/bin/env python3
"""
Binanceå†å²æ•°æ®ä¸‹è½½å™¨

ä» https://data.binance.vision/ æ‰¹é‡ä¸‹è½½å†å²Kçº¿æ•°æ®
æ”¯æŒå¤šå¸ç§ã€å¤šæ—¶é—´å‘¨æœŸã€å¤šæœˆä»½çš„æ•°æ®è·å–
"""

import os
import sys
import requests
import zipfile
import pandas as pd
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import time
import logging
from pathlib import Path
import hashlib

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('binance_download.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class BinanceHistoricalDownloader:
    """Binanceå†å²æ•°æ®ä¸‹è½½å™¨"""
    
    def __init__(self, data_dir: str = "binance_historical_data"):
        """
        åˆå§‹åŒ–ä¸‹è½½å™¨
        
        Args:
            data_dir: æ•°æ®ä¿å­˜ç›®å½•
        """
        self.base_url = "https://data.binance.vision/data/spot/monthly/klines"
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        self.downloads_dir = self.data_dir / "downloads"
        self.processed_dir = self.data_dir / "processed"
        self.downloads_dir.mkdir(exist_ok=True)
        self.processed_dir.mkdir(exist_ok=True)
        
        # è¯·æ±‚ä¼šè¯
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Binance Historical Data Downloader)'
        })
        
        # æ•°æ®åˆ—å
        self.columns = [
            'open_time', 'open', 'high', 'low', 'close', 'volume',
            'close_time', 'quote_volume', 'trades_count', 
            'taker_buy_base_volume', 'taker_buy_quote_volume', 'ignore'
        ]
    
    def generate_download_urls(self, symbol: str, interval: str, 
                             start_year: int, end_year: int) -> List[Dict]:
        """
        ç”Ÿæˆä¸‹è½½URLåˆ—è¡¨
        
        Args:
            symbol: äº¤æ˜“å¯¹ç¬¦å· (å¦‚ BTCUSDT)
            interval: æ—¶é—´é—´éš” (å¦‚ 1h, 1d)
            start_year: å¼€å§‹å¹´ä»½
            end_year: ç»“æŸå¹´ä»½
            
        Returns:
            List[Dict]: åŒ…å«URLå’Œæ–‡ä»¶ä¿¡æ¯çš„åˆ—è¡¨
        """
        urls = []
        
        for year in range(start_year, end_year + 1):
            for month in range(1, 13):
                # æ ¼å¼åŒ–æœˆä»½
                month_str = f"{year}-{month:02d}"
                
                # æ„å»ºURL
                filename = f"{symbol}-{interval}-{month_str}.zip"
                url = f"{self.base_url}/{symbol}/{interval}/{filename}"
                
                # æœ¬åœ°æ–‡ä»¶è·¯å¾„
                local_path = self.downloads_dir / filename
                
                urls.append({
                    'url': url,
                    'filename': filename,
                    'local_path': local_path,
                    'symbol': symbol,
                    'interval': interval,
                    'year': year,
                    'month': month
                })
        
        return urls
    
    def download_file(self, url: str, local_path: Path, 
                     timeout: int = 30) -> bool:
        """
        ä¸‹è½½å•ä¸ªæ–‡ä»¶
        
        Args:
            url: ä¸‹è½½URL
            local_path: æœ¬åœ°ä¿å­˜è·¯å¾„
            timeout: è¶…æ—¶æ—¶é—´
            
        Returns:
            bool: ä¸‹è½½æ˜¯å¦æˆåŠŸ
        """
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            if local_path.exists():
                logger.info(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {local_path.name}")
                return True
            
            logger.info(f"å¼€å§‹ä¸‹è½½: {url}")
            
            # å‘é€è¯·æ±‚
            response = self.session.get(url, timeout=timeout, stream=True)
            
            if response.status_code == 200:
                # ä¿å­˜æ–‡ä»¶
                with open(local_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)
                
                logger.info(f"âœ… ä¸‹è½½æˆåŠŸ: {local_path.name} ({local_path.stat().st_size / 1024:.2f} KB)")
                return True
                
            elif response.status_code == 404:
                logger.warning(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {url}")
                return False
                
            else:
                logger.error(f"âŒ ä¸‹è½½å¤±è´¥: {url} (çŠ¶æ€ç : {response.status_code})")
                return False
                
        except Exception as e:
            logger.error(f"âŒ ä¸‹è½½å¼‚å¸¸: {url} - {str(e)}")
            return False
    
    def extract_and_process_zip(self, zip_path: Path) -> Optional[pd.DataFrame]:
        """
        è§£å‹å¹¶å¤„ç†ZIPæ–‡ä»¶
        
        Args:
            zip_path: ZIPæ–‡ä»¶è·¯å¾„
            
        Returns:
            Optional[pd.DataFrame]: å¤„ç†åçš„æ•°æ®
        """
        try:
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                # è·å–CSVæ–‡ä»¶å
                csv_files = [f for f in zip_ref.namelist() if f.endswith('.csv')]
                
                if not csv_files:
                    logger.warning(f"ZIPæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°CSVæ–‡ä»¶: {zip_path}")
                    return None
                
                # è¯»å–ç¬¬ä¸€ä¸ªCSVæ–‡ä»¶
                csv_file = csv_files[0]
                
                with zip_ref.open(csv_file) as f:
                    # è¯»å–æ•°æ®
                    df = pd.read_csv(f, names=self.columns)
                    
                    # è½¬æ¢æ—¶é—´æˆ³
                    df['open_time'] = pd.to_datetime(df['open_time'], unit='ms')
                    df['close_time'] = pd.to_datetime(df['close_time'], unit='ms')
                    
                    # é‡å‘½ååˆ—ä»¥åŒ¹é…æˆ‘ä»¬çš„æ ¼å¼
                    df = df.rename(columns={
                        'open_time': 'timestamp',
                        'quote_volume': 'quote_volume',
                        'trades_count': 'trades_count'
                    })
                    
                    # é€‰æ‹©éœ€è¦çš„åˆ—
                    df = df[['timestamp', 'open', 'high', 'low', 'close', 'volume', 'trades_count']]
                    
                    # æ·»åŠ symbolåˆ—
                    symbol = zip_path.stem.split('-')[0]
                    df['symbol'] = symbol
                    
                    logger.info(f"âœ… å¤„ç†å®Œæˆ: {zip_path.name} ({len(df)} æ¡è®°å½•)")
                    return df
                    
        except Exception as e:
            logger.error(f"âŒ å¤„ç†ZIPæ–‡ä»¶å¤±è´¥: {zip_path} - {str(e)}")
            return None
    
    def download_symbol_data(self, symbol: str, interval: str, 
                           start_year: int = 2020, end_year: int = 2024,
                           delay: float = 0.1) -> bool:
        """
        ä¸‹è½½æŒ‡å®šäº¤æ˜“å¯¹çš„æ•°æ®
        
        Args:
            symbol: äº¤æ˜“å¯¹ç¬¦å·
            interval: æ—¶é—´é—´éš”
            start_year: å¼€å§‹å¹´ä»½
            end_year: ç»“æŸå¹´ä»½
            delay: è¯·æ±‚é—´éš”
            
        Returns:
            bool: ä¸‹è½½æ˜¯å¦æˆåŠŸ
        """
        logger.info(f"ğŸš€ å¼€å§‹ä¸‹è½½ {symbol} {interval} æ•°æ® ({start_year}-{end_year})")
        
        # ç”Ÿæˆä¸‹è½½URL
        urls = self.generate_download_urls(symbol, interval, start_year, end_year)
        
        # ä¸‹è½½ç»Ÿè®¡
        downloaded_files = []
        failed_files = []
        
        for url_info in urls:
            # ä¸‹è½½æ–‡ä»¶
            success = self.download_file(url_info['url'], url_info['local_path'])
            
            if success:
                downloaded_files.append(url_info)
            else:
                failed_files.append(url_info)
            
            # è¯·æ±‚é—´éš”
            time.sleep(delay)
        
        logger.info(f"ğŸ“Š ä¸‹è½½å®Œæˆ: {len(downloaded_files)} æˆåŠŸ, {len(failed_files)} å¤±è´¥")
        
        # å¤„ç†ä¸‹è½½çš„æ–‡ä»¶
        if downloaded_files:
            self.process_downloaded_files(downloaded_files, symbol, interval)
        
        return len(downloaded_files) > 0
    
    def process_downloaded_files(self, downloaded_files: List[Dict], 
                               symbol: str, interval: str) -> None:
        """
        å¤„ç†ä¸‹è½½çš„æ–‡ä»¶ï¼Œåˆå¹¶ä¸ºå®Œæ•´æ•°æ®é›†
        
        Args:
            downloaded_files: ä¸‹è½½æˆåŠŸçš„æ–‡ä»¶åˆ—è¡¨
            symbol: äº¤æ˜“å¯¹ç¬¦å·
            interval: æ—¶é—´é—´éš”
        """
        logger.info(f"ğŸ“¦ å¼€å§‹å¤„ç† {symbol} {interval} æ•°æ®...")
        
        all_data = []
        
        for file_info in downloaded_files:
            if file_info['local_path'].exists():
                df = self.extract_and_process_zip(file_info['local_path'])
                if df is not None:
                    all_data.append(df)
        
        if all_data:
            # åˆå¹¶æ‰€æœ‰æ•°æ®
            combined_df = pd.concat(all_data, ignore_index=True)
            
            # æŒ‰æ—¶é—´æ’åº
            combined_df = combined_df.sort_values('timestamp').reset_index(drop=True)
            
            # å»é‡
            combined_df = combined_df.drop_duplicates(subset=['timestamp']).reset_index(drop=True)
            
            # ä¿å­˜å¤„ç†åçš„æ•°æ®
            output_file = self.processed_dir / f"{symbol}_{interval}_combined.csv"
            combined_df.to_csv(output_file, index=False)
            
            logger.info(f"âœ… åˆå¹¶å®Œæˆ: {output_file}")
            logger.info(f"ğŸ“Š æ•°æ®ç»Ÿè®¡: {len(combined_df)} æ¡è®°å½•")
            logger.info(f"ğŸ“… æ—¶é—´èŒƒå›´: {combined_df['timestamp'].min()} åˆ° {combined_df['timestamp'].max()}")
            
            # æ˜¾ç¤ºæ•°æ®è´¨é‡ä¿¡æ¯
            self.show_data_quality(combined_df, symbol, interval)
        
        else:
            logger.warning(f"âš ï¸ æ²¡æœ‰å¯å¤„ç†çš„æ•°æ®: {symbol} {interval}")
    
    def show_data_quality(self, df: pd.DataFrame, symbol: str, interval: str) -> None:
        """
        æ˜¾ç¤ºæ•°æ®è´¨é‡ä¿¡æ¯
        
        Args:
            df: æ•°æ®æ¡†
            symbol: äº¤æ˜“å¯¹ç¬¦å·
            interval: æ—¶é—´é—´éš”
        """
        logger.info(f"ğŸ“‹ {symbol} {interval} æ•°æ®è´¨é‡æŠ¥å‘Š:")
        logger.info(f"   è®°å½•æ•°: {len(df):,}")
        logger.info(f"   æ—¶é—´è·¨åº¦: {(df['timestamp'].max() - df['timestamp'].min()).days} å¤©")
        logger.info(f"   æ•°æ®å®Œæ•´æ€§: {(1 - df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100:.2f}%")
        logger.info(f"   å¹³å‡æ—¥äº¤æ˜“é‡: {df['volume'].mean():.2f}")
        logger.info(f"   ä»·æ ¼èŒƒå›´: {df['low'].min():.2f} - {df['high'].max():.2f}")
    
    def download_multiple_symbols(self, symbols: List[str], intervals: List[str],
                                start_year: int = 2020, end_year: int = 2024) -> Dict:
        """
        æ‰¹é‡ä¸‹è½½å¤šä¸ªäº¤æ˜“å¯¹çš„æ•°æ®
        
        Args:
            symbols: äº¤æ˜“å¯¹åˆ—è¡¨
            intervals: æ—¶é—´é—´éš”åˆ—è¡¨
            start_year: å¼€å§‹å¹´ä»½
            end_year: ç»“æŸå¹´ä»½
            
        Returns:
            Dict: ä¸‹è½½ç»“æœç»Ÿè®¡
        """
        results = {
            'successful': [],
            'failed': []
        }
        
        total_tasks = len(symbols) * len(intervals)
        current_task = 0
        
        for symbol in symbols:
            for interval in intervals:
                current_task += 1
                logger.info(f"ğŸ¯ è¿›åº¦: {current_task}/{total_tasks} - {symbol} {interval}")
                
                try:
                    success = self.download_symbol_data(symbol, interval, start_year, end_year)
                    
                    if success:
                        results['successful'].append(f"{symbol}_{interval}")
                    else:
                        results['failed'].append(f"{symbol}_{interval}")
                        
                except Exception as e:
                    logger.error(f"âŒ ä¸‹è½½å¤±è´¥: {symbol} {interval} - {str(e)}")
                    results['failed'].append(f"{symbol}_{interval}")
        
        # æ˜¾ç¤ºæ€»ç»“
        logger.info(f"\nğŸ“Š æ‰¹é‡ä¸‹è½½å®Œæˆ:")
        logger.info(f"âœ… æˆåŠŸ: {len(results['successful'])} ä¸ª")
        logger.info(f"âŒ å¤±è´¥: {len(results['failed'])} ä¸ª")
        
        if results['successful']:
            logger.info(f"âœ… æˆåŠŸé¡¹ç›®: {', '.join(results['successful'])}")
        
        if results['failed']:
            logger.info(f"âŒ å¤±è´¥é¡¹ç›®: {', '.join(results['failed'])}")
        
        return results

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Binanceå†å²æ•°æ®ä¸‹è½½å™¨")
    print("=" * 60)
    
    # åˆ›å»ºä¸‹è½½å™¨
    downloader = BinanceHistoricalDownloader()
    
    # é…ç½®ä¸‹è½½å‚æ•°
    symbols = ['BTCUSDT', 'ETHUSDT', 'SOLUSDT', 'ADAUSDT', 'DOGEUSDT']
    intervals = ['1h', '1d']
    start_year = 2020
    end_year = 2024  # åˆ°2024å¹´
    
    print(f"ğŸ“Š ä¸‹è½½é…ç½®:")
    print(f"   äº¤æ˜“å¯¹: {', '.join(symbols)}")
    print(f"   æ—¶é—´å‘¨æœŸ: {', '.join(intervals)}")
    print(f"   æ—¶é—´èŒƒå›´: {start_year}-{end_year}")
    print(f"   ä¿å­˜ç›®å½•: {downloader.data_dir}")
    print()
    
    # å¼€å§‹æ‰¹é‡ä¸‹è½½
    results = downloader.download_multiple_symbols(symbols, intervals, start_year, end_year)
    
    print(f"\nğŸ‰ ä¸‹è½½å®Œæˆ!")
    print(f"ğŸ“ æ•°æ®ä¿å­˜åœ¨: {downloader.processed_dir}")
    
    return results

if __name__ == "__main__":
    results = main()